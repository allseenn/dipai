{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0463d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "SETS = './sets/' # папка с файлами с наборами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f96818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_serializer(obj): # для правильного преобразования даты в ISO формат\n",
    "    if isinstance(obj, (date)):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')\n",
    "    \n",
    "\n",
    "def load_dataset(filename): \n",
    "    if os.path.exists(SETS + filename + '.gz'):\n",
    "        with gzip.open(SETS + filename + '.gz', 'rb') as gzip_ref:\n",
    "            return pd.DataFrame(json.load(gzip_ref))\n",
    "        \n",
    "    \n",
    "def save_dataset(filename):\n",
    "    data = globals().get(filename)\n",
    "    if data is not None:\n",
    "        data = data.to_dict(orient='records')\n",
    "        json_data = json.dumps(data, ensure_ascii=False, default=default_serializer)\n",
    "        with gzip.open(SETS + filename + '.gz', 'wb') as gzip_file:\n",
    "            gzip_file.write(json_data.encode('utf-8'))\n",
    "        print(f'Сохранено {len(data)} записей в {filename}.gz')\n",
    "        \n",
    "pd.set_option('display.max_colwidth', None) # для отображения полного текста в ячейках\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7638e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "exams_ecology = load_dataset('exams_ecology')\n",
    "exams_ecology['year'] = pd.to_datetime(exams_ecology['start']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c456329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.1224281170699344\n",
      "MAE: 0.8747404421361281\n",
      "R2: -0.014977280786252267\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load data\n",
    "data = exams_ecology\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['global_id', 'year', 'stroi', 'roads']]\n",
    "y = data['stupid']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a more complex neural network\n",
    "class ImprovedNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedNeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 512)  # Increased neurons in the first layer\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.4)  # Increased dropout rate\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 8)\n",
    "        self.fc8 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = torch.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = self.fc8(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the improved model\n",
    "model = ImprovedNeuralNetwork()\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001  # Lower learning rate\n",
    "epochs = 500  # Increase the number of epochs\n",
    "\n",
    "# Initialize loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Predict on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions to numpy for metric evaluation\n",
    "y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "# Evaluate the model\n",
    "mse_improved = mean_squared_error(y_test, y_pred)\n",
    "mae_improved = mean_absolute_error(y_test, y_pred)\n",
    "r2_improved = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse_improved}')\n",
    "print(f'MAE: {mae_improved}')\n",
    "print(f'R2: {r2_improved}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346d5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import tensorflow as tf\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Load data\n",
    "# data = exams_ecology\n",
    "\n",
    "# # Define features and target variable\n",
    "# X = data[['global_id', 'year', 'stroi', 'roads']]\n",
    "# y = data['stupid']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Scale the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Build the TensorFlow model\n",
    "# def build_model():\n",
    "#     model = tf.keras.Sequential([\n",
    "#         tf.keras.layers.Dense(512, input_shape=(4,), activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "#         tf.keras.layers.Dense(256, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.4),\n",
    "        \n",
    "#         tf.keras.layers.Dense(128, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "#         tf.keras.layers.Dense(64, activation='relu'),\n",
    "#         tf.keras.layers.BatchNormalization(),\n",
    "#         tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "#         tf.keras.layers.Dense(32, activation='relu'),\n",
    "#         tf.keras.layers.Dense(16, activation='relu'),\n",
    "#         tf.keras.layers.Dense(8, activation='relu'),\n",
    "#         tf.keras.layers.Dense(1)  # Output layer\n",
    "#     ])\n",
    "    \n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "#     model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Initialize the model\n",
    "# model = build_model()\n",
    "\n",
    "# # Callbacks\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train_scaled, y_train,\n",
    "#     epochs=200,\n",
    "#     batch_size=32,\n",
    "#     validation_split=0.2,\n",
    "#     callbacks=[early_stopping, reduce_lr],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse_tf = mean_squared_error(y_test, y_pred)\n",
    "# mae_tf = mean_absolute_error(y_test, y_pred)\n",
    "# r2_tf = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'MSE (TensorFlow): {mse_tf}')\n",
    "# print(f'MAE (TensorFlow): {mae_tf}')\n",
    "# print(f'R2 (TensorFlow): {r2_tf}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
