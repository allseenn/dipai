{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s82sYn_PM39K"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Add, Activation, LSTM, Flatten, Dense, Dropout, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqc7TbLVNIxG",
        "outputId": "9e4731f6-92d7-4695-d5d3-ba0ca412fa8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 20:09:38--  https://github.com/allseenn/dipai/raw/main/exams_ecology.csv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/allseenn/dipai/main/exams_ecology.csv [following]\n",
            "--2024-08-12 20:09:38--  https://raw.githubusercontent.com/allseenn/dipai/main/exams_ecology.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1102554 (1.1M) [text/plain]\n",
            "Saving to: ‘exams_ecology.csv’\n",
            "\n",
            "exams_ecology.csv   100%[===================>]   1.05M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-08-12 20:09:39 (22.4 MB/s) - ‘exams_ecology.csv’ saved [1102554/1102554]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://github.com/allseenn/dipai/raw/main/exams_ecology.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IwovAek8NTE4"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv('./exams_ecology.csv')\n",
        "data['start'] = pd.to_datetime(data['start'])\n",
        "data['end'] = pd.to_datetime(data['end'])\n",
        "data['start'] = data['start'].apply(lambda x: int(dt.datetime.timestamp(x)))\n",
        "data['end'] = data['end'].apply(lambda x: int(dt.datetime.timestamp(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmxEu52HNgGu",
        "outputId": "42664354-88be-4a6c-eac6-bb3782b305c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 49ms/step - loss: 6.7981 - val_loss: 1.9342\n",
            "Epoch 2/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 3.2227 - val_loss: 1.5756\n",
            "Epoch 3/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.6297 - val_loss: 1.4874\n",
            "Epoch 4/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 3.2206 - val_loss: 1.3087\n",
            "Epoch 5/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.8766 - val_loss: 1.2843\n",
            "Epoch 6/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - loss: 2.5954 - val_loss: 1.2799\n",
            "Epoch 7/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.5051 - val_loss: 1.2676\n",
            "Epoch 8/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.3170 - val_loss: 1.2955\n",
            "Epoch 9/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 2.3534 - val_loss: 1.2658\n",
            "Epoch 10/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.3004 - val_loss: 1.3036\n",
            "Epoch 11/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - loss: 2.5616 - val_loss: 1.2612\n",
            "Epoch 12/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.4830 - val_loss: 1.2617\n",
            "Epoch 13/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.3112 - val_loss: 1.3627\n",
            "Epoch 14/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - loss: 2.4675 - val_loss: 1.2624\n",
            "Epoch 15/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.5718 - val_loss: 1.2695\n",
            "Epoch 16/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.2518 - val_loss: 1.2804\n",
            "Epoch 17/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - loss: 2.4864 - val_loss: 1.2800\n",
            "Epoch 18/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.6754 - val_loss: 1.2677\n",
            "Epoch 19/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.3247 - val_loss: 1.3418\n",
            "Epoch 20/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 1.9857 - val_loss: 1.2691\n",
            "Epoch 21/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 45ms/step - loss: 2.4201 - val_loss: 1.2619\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step\n",
            "Epoch 1/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 50ms/step - loss: 6.8132 - val_loss: 1.9724\n",
            "Epoch 2/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - loss: 2.7345 - val_loss: 1.5137\n",
            "Epoch 3/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.8177 - val_loss: 1.3911\n",
            "Epoch 4/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.7128 - val_loss: 1.3164\n",
            "Epoch 5/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.6327 - val_loss: 1.2957\n",
            "Epoch 6/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.0861 - val_loss: 1.2756\n",
            "Epoch 7/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.3564 - val_loss: 1.2674\n",
            "Epoch 8/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.9675 - val_loss: 1.2689\n",
            "Epoch 9/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.0715 - val_loss: 1.2731\n",
            "Epoch 10/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.2189 - val_loss: 1.2688\n",
            "Epoch 11/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - loss: 2.4368 - val_loss: 1.2679\n",
            "Epoch 12/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.7993 - val_loss: 1.2642\n",
            "Epoch 13/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.0485 - val_loss: 1.2640\n",
            "Epoch 14/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - loss: 2.4623 - val_loss: 1.2802\n",
            "Epoch 15/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - loss: 2.3699 - val_loss: 1.2608\n",
            "Epoch 16/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - loss: 2.0515 - val_loss: 1.3277\n",
            "Epoch 17/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - loss: 2.3751 - val_loss: 1.2800\n",
            "Epoch 18/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.1013 - val_loss: 1.3820\n",
            "Epoch 19/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.1686 - val_loss: 1.2678\n",
            "Epoch 20/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.1659 - val_loss: 1.2721\n",
            "Epoch 21/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.6435 - val_loss: 1.2772\n",
            "Epoch 22/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.1697 - val_loss: 1.3037\n",
            "Epoch 23/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.0602 - val_loss: 1.2643\n",
            "Epoch 24/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.4554 - val_loss: 1.2722\n",
            "Epoch 25/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.7181 - val_loss: 1.2643\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
            "Epoch 1/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - loss: 6.4972 - val_loss: 1.8992\n",
            "Epoch 2/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 3.1734 - val_loss: 1.4765\n",
            "Epoch 3/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.8545 - val_loss: 1.3632\n",
            "Epoch 4/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.2466 - val_loss: 1.3113\n",
            "Epoch 5/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - loss: 2.0768 - val_loss: 1.2897\n",
            "Epoch 6/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - loss: 2.6709 - val_loss: 1.2984\n",
            "Epoch 7/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - loss: 2.6790 - val_loss: 1.2712\n",
            "Epoch 8/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.6712 - val_loss: 1.2694\n",
            "Epoch 9/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - loss: 3.5033 - val_loss: 1.2643\n",
            "Epoch 10/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.2343 - val_loss: 1.2748\n",
            "Epoch 11/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.7323 - val_loss: 1.3006\n",
            "Epoch 12/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 3.2672 - val_loss: 1.2943\n",
            "Epoch 13/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.1104 - val_loss: 1.2675\n",
            "Epoch 14/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.1371 - val_loss: 1.2859\n",
            "Epoch 15/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.5046 - val_loss: 1.2756\n",
            "Epoch 16/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - loss: 2.3467 - val_loss: 1.2644\n",
            "Epoch 17/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - loss: 2.5601 - val_loss: 1.2608\n",
            "Epoch 18/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.6661 - val_loss: 1.2648\n",
            "Epoch 19/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.2452 - val_loss: 1.3448\n",
            "Epoch 20/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 46ms/step - loss: 2.3030 - val_loss: 1.2876\n",
            "Epoch 21/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.2749 - val_loss: 1.2645\n",
            "Epoch 22/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.6774 - val_loss: 1.2613\n",
            "Epoch 23/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.7790 - val_loss: 1.2689\n",
            "Epoch 24/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.1904 - val_loss: 1.2755\n",
            "Epoch 25/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 46ms/step - loss: 2.3767 - val_loss: 1.2609\n",
            "Epoch 26/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.5417 - val_loss: 1.2716\n",
            "Epoch 27/3000\n",
            "\u001b[1m281/281\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 46ms/step - loss: 2.5072 - val_loss: 1.2661\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step\n",
            "Ensemble Model Evaluation:\n",
            "Mean Squared Error (MSE): 2.1038\n",
            "Mean Absolute Error (MAE): 0.8661\n",
            "Coefficient of Determination (R^2): -0.0060\n",
            "CPU times: user 17min 19s, sys: 34.2 s, total: 17min 53s\n",
            "Wall time: 21min 48s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Calculate combined ecological indices using the provided formula\n",
        "ecological_columns = data.columns[3:21]\n",
        "data['combined_ecological_index'] = data[ecological_columns].apply(lambda x: 1 - np.prod(x / 1000), axis=1)\n",
        "\n",
        "# Prepare the feature set including 'global_id' as an important feature\n",
        "train_columns = data.columns.difference(['stupid'])\n",
        "num_features = len(train_columns)\n",
        "\n",
        "X = data[train_columns].values\n",
        "y = data['stupid'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape data for the model, but note it should fit the 1D convolutions and LSTMs\n",
        "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Transformer block\n",
        "def transformer_block(x, num_heads, ff_dim, dropout_rate):\n",
        "    # Multi-Head Attention\n",
        "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim, dropout=dropout_rate)(x, x)\n",
        "    attention_output = Dropout(dropout_rate)(attention_output)\n",
        "    out1 = Add()([x, attention_output])\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(x.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "    out2 = Add()([out1, ffn_output])\n",
        "    return LayerNormalization(epsilon=1e-6)(out2)\n",
        "\n",
        "# Build a super advanced model\n",
        "def build_super_advanced_model(num_features):\n",
        "    inputs = Input(shape=(num_features, 1))\n",
        "\n",
        "    # Initial Convolutional Layers\n",
        "    x = Conv1D(256, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(512, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Transformer Encoder Layers\n",
        "    for _ in range(4):\n",
        "        x = transformer_block(x, num_heads=4, ff_dim=256, dropout_rate=0.3)\n",
        "\n",
        "    # LSTM Layers with increased units\n",
        "    x = LSTM(512, return_sequences=True)(x)\n",
        "    x = LSTM(256, return_sequences=True)(x)\n",
        "    x = LSTM(128, return_sequences=True)(x)\n",
        "\n",
        "    # Global Average Pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense layers with increased neurons and regularization\n",
        "    x = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Ensemble learning: train multiple models and average predictions\n",
        "def train_and_evaluate_ensemble(X_train, y_train, X_test, y_test, num_models=5):\n",
        "    predictions = []\n",
        "    for _ in range(num_models):\n",
        "        model = build_super_advanced_model(num_features)\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=3000,\n",
        "            batch_size=16,\n",
        "            validation_split=0.1,\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "    # Average predictions from all models\n",
        "    y_pred_ensemble = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Evaluate\n",
        "    mse_tf = mean_squared_error(y_test, y_pred_ensemble)\n",
        "    mae_tf = mean_absolute_error(y_test, y_pred_ensemble)\n",
        "\n",
        "    print(\"Ensemble Model Evaluation:\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse_tf:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae_tf:.4f}\")\n",
        "    print(f\"Coefficient of Determination (R^2): {r2_score(y_test, y_pred_ensemble):.4f}\")\n",
        "\n",
        "# Train and evaluate the ensemble\n",
        "train_and_evaluate_ensemble(X_train_scaled, y_train, X_test_scaled, y_test, num_models=3)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}